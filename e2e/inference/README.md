## Intel AI Inference E2E Solution for OpenShift

### Overview
Intel AI inference e2e solution for OCP is built upon Intel® dGPU provisioning for OpenShift and Intel® Xeon® processors. The two following AI inference modes are used to test with the Intel Data Center GPU Card provisioning:
* **Interactive Mode**
[Open Data Hub (ODH)](https://github.com/opendatahub-io) and [Red Hat OpenShift Data Science (RHODS)](https://www.redhat.com/en/technologies/cloud-computing/openshift/openshift-data-science) provides Intel® OpenVINO™ based [Jupiter Notebook](https://jupyter.org/) to help users interactively debug the inferencing applications or optimize the models with OCP using Intel Data Center GPU cards and Intel Xeon processors.
*	**Deployment Mode**
[Intel OpenVINO™ Toolkit](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html) and [Operator](https://github.com/openvinotoolkit/operator) provide the [OpenVINO Model Server (OVMS)](https://github.com/openvinotoolkit/model_server) for users to deploy their inferencing workload using Intel Data Center GPU cards and Intel Xeon processors on OCP cluster in cloud or edge environment.

     `note: The verification on this mode is ongoing`

### Deploy Intel AI Inference E2E Solution

* **Install RHODS on OpenShift**
* **Install Intel OpenVINO Toolkit Operator**

### Run Interactive Mode Demo

### Run Deployment Mode Demo 
